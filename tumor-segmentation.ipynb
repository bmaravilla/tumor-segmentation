{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bmaravilla/tumor-segmentation/blob/main/tumor-segmentation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kruimFzJbEhc"
      },
      "source": [
        "# Problem definition: Segmentation of gliomas in MRI scans\n",
        "\n",
        "In this jupyter notebook, we will be doing whole tumor (WT) segmentation. WT includes subregions 1, 2 and 4. \n",
        "\n",
        "\n",
        "\n",
        "Each pixel on image is labelled as :\n",
        "\n",
        "- Pixel is part of a tumor area (subregions: 1, 2, 4)\n",
        "- Pixel is not a part of tumor area (0)\n",
        "\n",
        "where, 0 - no tumor, 1 - necrotic/core/ non-enhancing, 2 - edema, 4 - enhancing\n",
        "\n",
        "\n",
        "**Objectivies (What will we learn here?)**\n",
        "1. Setting up the env\n",
        "2. Create training, validation and test ids\n",
        "3. Image pre-processing\n",
        "4. Performance metrics for semantic segmentation\n",
        "5. Loss functions for semantic segmentation\n",
        "6. Building the Unet model\n",
        "7. Training the model\n",
        "8. Looking at the learning curve\n",
        "9. Predicting using the Unet model\n",
        "10. What next? It is time to play!\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UEV8zsL38AbI"
      },
      "source": [
        "## Step 1: Set up the env\n",
        "\n",
        "Download all the packages needed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S2ui_lqWcDXz",
        "outputId": "eee33b11-ce6b-4678-f7d6-9bdc2f316f29"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.12.0\n"
          ]
        }
      ],
      "source": [
        "# this notebook is saved in github\n",
        "import os\n",
        "import numpy as np\n",
        "from numpy import expand_dims\n",
        "import cv2\n",
        "import nibabel as nib \n",
        "import matplotlib.pyplot as plt\n",
        "import glob\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "from google.colab import drive\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras import layers\n",
        "import keras.backend as K\n",
        "from keras.models import Model\n",
        "from keras.layers import Input, concatenate, Conv2D, MaxPooling2D, Activation, UpSampling2D, BatchNormalization, Dropout\n",
        "from tensorflow.keras.layers.experimental import preprocessing \n",
        "from tensorflow.keras.optimizers import *\n",
        "from sklearn.model_selection import train_test_split\n",
        "from IPython.display import clear_output\n",
        "\n",
        "tf.config.run_functions_eagerly(True)\n",
        "\n",
        "print(tf.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r0VPeE7kMIgW"
      },
      "source": [
        "Because we are working on data stored in Google Cloud Storage bucket, the following steps will need to be taken:\n",
        "\n",
        "1. Install [gcsfuse](https://colab.research.google.com/drive/19slw0Fl_kDJmaL5NIZ71bxagv5vxq55v#scrollTo=r0VPeE7kMIgW&line=3&uniqifier=1) it is a package used to mount the cloud bucket on to your local environment.\n",
        "2. Authenticate your google account\n",
        "3. Create a new directory to mount the storage bucket\n",
        "4. Use gcsfuse to mount the storage bucket\n",
        "\n",
        "You can browse throught the storage bucket here"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zXVjl4PPMqCO",
        "outputId": "20e53897-d265-4128-d1fb-2e69e623a7bd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r0% [Working]\r            \rGet:1 https://cloud.r-project.org/bin/linux/ubuntu focal-cran40/ InRelease [3,622 B]\n",
            "\r0% [Waiting for headers] [Connecting to security.ubuntu.com (185.125.190.36)] [\r                                                                               \rHit:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64  InRelease\n",
            "\r0% [Waiting for headers] [Connecting to security.ubuntu.com (185.125.190.36)] [\r                                                                               \rHit:3 http://archive.ubuntu.com/ubuntu focal InRelease\n",
            "Get:4 http://archive.ubuntu.com/ubuntu focal-updates InRelease [114 kB]\n",
            "Get:5 http://security.ubuntu.com/ubuntu focal-security InRelease [114 kB]\n",
            "Hit:6 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu focal InRelease\n",
            "Get:7 http://archive.ubuntu.com/ubuntu focal-backports InRelease [108 kB]\n",
            "Hit:8 http://ppa.launchpad.net/cran/libgit2/ubuntu focal InRelease\n",
            "Hit:9 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu focal InRelease\n",
            "Get:10 https://cloud.r-project.org/bin/linux/ubuntu focal-cran40/ Packages [77.6 kB]\n",
            "Hit:11 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu focal InRelease\n",
            "Hit:12 http://ppa.launchpad.net/ubuntugis/ppa/ubuntu focal InRelease\n",
            "Get:13 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 Packages [3,150 kB]\n",
            "Get:14 http://archive.ubuntu.com/ubuntu focal-updates/universe amd64 Packages [1,334 kB]\n",
            "Fetched 4,901 kB in 5s (1,075 kB/s)\n",
            "Reading package lists... Done\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "fuse is already the newest version (2.9.9-3).\n",
            "lsb-release is already the newest version (11.1.0ubuntu2).\n",
            "lsb-release set to manually installed.\n",
            "0 upgraded, 0 newly installed, 0 to remove and 25 not upgraded.\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
            "100 12.7M  100 12.7M    0     0  22.4M      0 --:--:-- --:--:-- --:--:-- 22.4M\n",
            "Selecting previously unselected package gcsfuse.\n",
            "(Reading database ... 122518 files and directories currently installed.)\n",
            "Preparing to unpack gcsfuse_0.41.12_amd64.deb ...\n",
            "Unpacking gcsfuse (0.41.12) ...\n",
            "Setting up gcsfuse (0.41.12) ...\n",
            "Hit:1 https://cloud.r-project.org/bin/linux/ubuntu focal-cran40/ InRelease\n",
            "Hit:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64  InRelease\n",
            "Hit:3 http://archive.ubuntu.com/ubuntu focal InRelease\n",
            "Hit:4 http://security.ubuntu.com/ubuntu focal-security InRelease\n",
            "Hit:5 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu focal InRelease\n",
            "Hit:6 http://archive.ubuntu.com/ubuntu focal-updates InRelease\n",
            "Hit:7 http://archive.ubuntu.com/ubuntu focal-backports InRelease\n",
            "Hit:8 http://ppa.launchpad.net/cran/libgit2/ubuntu focal InRelease\n",
            "Hit:9 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu focal InRelease\n",
            "Hit:10 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu focal InRelease\n",
            "Hit:11 http://ppa.launchpad.net/ubuntugis/ppa/ubuntu focal InRelease\n",
            "Reading package lists... Done\n"
          ]
        }
      ],
      "source": [
        "!apt-get update -y && apt-get install -y fuse lsb-release; \\\n",
        "    curl -L -O https://github.com/GoogleCloudPlatform/gcsfuse/releases/download/v0.41.12/gcsfuse_0.41.12_amd64.deb; \\\n",
        "    dpkg --install gcsfuse_0.41.12_amd64.deb; \\\n",
        "    apt-get update; \\\n",
        "    apt-get clean;"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_E-Z61lpMyaD"
      },
      "source": [
        "Now call google authentication with the following code block and allow access in the pop-up window\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "BDMYmN6YM0oZ"
      },
      "outputs": [],
      "source": [
        "from google.colab import auth\n",
        "auth.authenticate_user()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YWEH8db2M81O"
      },
      "source": [
        "Let's set the project id and storage bucket id"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cs-99226M_CM",
        "outputId": "c267fa16-fdee-4fa2-b22d-8047f1b6786b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;33mWARNING:\u001b[0m You do not appear to have access to project [sfsu-378805] or it does not exist.\n",
            "Are you sure you wish to set property [core/project] to sfsu-378805?\n",
            "\n",
            "Do you want to continue (Y/n)?  "
          ]
        }
      ],
      "source": [
        "\n",
        "project_id = 'sfsu-378805'\n",
        "bucket_name = 'csc-509-image-files'\n",
        "!gcloud config set project {project_id}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8OqwEjm8NEb8"
      },
      "source": [
        "You can list using gsutil to see if we have the correct storage"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "94auMqifNHKQ"
      },
      "outputs": [],
      "source": [
        "!gsutil ls"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GPedtcKlOhFu"
      },
      "source": [
        "Let's make a new directory called images to mount our bucket"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v6mbADS1OjBN"
      },
      "outputs": [],
      "source": [
        "!mkdir images"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w1XWyZDzNQUt"
      },
      "source": [
        "Call gcsfuse to mount the bucket\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7QGrdbLVNSe1"
      },
      "outputs": [],
      "source": [
        "!gcsfuse --implicit-dirs csc-509-image-files images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GZm1aBH1NXjp"
      },
      "outputs": [],
      "source": [
        "!ls images/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qNkDjIJBP5pM"
      },
      "source": [
        "## Step 2: Now let's create the training, validation and test ids. \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VJyN3Q6ZeyPT"
      },
      "outputs": [],
      "source": [
        "# Let's define the train data path.\n",
        "TRAIN_DATA_PATH = Path('/content/images/Module1_BraTS/MICCAI_BraTS2020_TrainingData')\n",
        "\n",
        "# BraTS also has \"ValidationData\" which has only images and no segmentation masks. So we won't be using it in this notebook. \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Os9-EmvSSe-y"
      },
      "outputs": [],
      "source": [
        "# Let's create the training, validation and test ids.\n",
        "\n",
        "train_dir = [f.path for f in os.scandir(TRAIN_DATA_PATH) if f.is_dir()]\n",
        "\n",
        "# We will create train, validation and test ids from the train_dir dataset\n",
        "\n",
        "\n",
        "def list_to_ids(dir:str):\n",
        "    \"\"\"\n",
        "    Will convert the dir paths to ids by parsing the paths.\n",
        "    dir: string, image dir paths in BRATS\n",
        "    \"\"\"\n",
        "    x = []\n",
        "    for i in range(0,len(dir)):\n",
        "        x.append(dir[i].split('/')[-1])\n",
        "    return x\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kMzHrmghyyYI"
      },
      "source": [
        "How to do train-test-split using sklearn: https://www.stackvidhya.com/train-test-split-using-sklearn-in-python/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0pMQp1t4yi1B"
      },
      "outputs": [],
      "source": [
        "# Now let's use the defined function\n",
        "\n",
        "ids = list_to_ids(train_dir) \n",
        "\n",
        "# Split dataset to create training ids, validation ids and test ids\n",
        "# Here we have selected the size of test set as 20% which is a common practice.\n",
        "train_ids, test_ids = train_test_split(ids,test_size=0.2) \n",
        "\n",
        "# Create validation ids by further splitting the train ids, we again use 20% as size of valisation set. \n",
        "# Validation set is also referred to as tuning set. \n",
        "#doesn't read from the orginial total to the training dataset; 369 for training- 20% for validation = 295 traning imgs\n",
        "#now we must use a larger % to get equal amount of validation/test datasets\n",
        "\n",
        "train_ids, val_ids = train_test_split(train_ids,test_size=0.25) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Iy3tCNP4QY4L"
      },
      "outputs": [],
      "source": [
        "# Now looks at the number of patient ids in training, validation and test sets\n",
        "print(f'There are {len(train_ids)} patient ids in training set')\n",
        "print(f'There are {len(test_ids)} patient ids in test set')\n",
        "print(f'There are {len(val_ids)} patient ids in validation set')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XWZ62WTNVeBU"
      },
      "source": [
        "## Step 3: Let's do some image pre-processing. You have already learnt basics of reading and visualizing nifti images, selecting slices and looking at histograms.\n",
        "\n",
        "Here, we will look at some pre-processing steps specific to semantic segmentation. It's important to understand that the pro-processing steps could vary depending on the task at hand.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vat3lmKjWKDQ"
      },
      "outputs": [],
      "source": [
        "# Let's rewind and remember the type of images available: T1, T1CE, T2, FLAIR and corresponding masks\n",
        "\n",
        "# We will use the read_patient_niftis module we had learnt in the previous notebook\n",
        "def read_patient_niftis(patient_id: str, \n",
        "                        niftis_to_load = ['t1', 't1ce', 't2', 'flair', 'seg'], \n",
        "                        data_path = TRAIN_DATA_PATH):\n",
        "  \"\"\"\n",
        "  Will read in the images from a single patient and return a dictionary of \n",
        "  those images with the key as the image type and the value as the nifti object. \n",
        "  patient_id: string of format '001' through '369', patient ID in BRATS\n",
        "  niftis_to_load: default is list containing all the types of images that we care about; can sub in another list if desired. \n",
        "  data_path: default is DATA_PATH specified above with respect to the mounted google drive (in colab)\n",
        "\n",
        "  \"\"\"\n",
        "  patient_image_dict = {}\n",
        "  for image in niftis_to_load: \n",
        "    patient_image_dict[image]=nib.load(Path(data_path, f'{patient_id}', f'{patient_id}_{image}.nii.gz'))\n",
        "  return patient_image_dict\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z5k8obyuzQjM"
      },
      "outputs": [],
      "source": [
        "# Let's take a look one of the image and the corresponding segmentation mask from the training set before we proceed\n",
        "pt_img_dict = read_patient_niftis(train_ids[12])\n",
        "\n",
        "# Now lets visualize all the images side by side to get a sense of what's happening.\n",
        "plt.figure(figsize=(45,30)) # specifying the overall grid size\n",
        "for i, (key, value) in enumerate(pt_img_dict.items()):\n",
        "    plt.subplot(1, 5, i+1)\n",
        "    plt.imshow(value._dataobj[:, :, value.shape[-1]//2].T, cmap='gray') # Looks halfway through the volume and transposes the image so that its facing upward. \n",
        "    plt.axis('off')\n",
        "    plt.title(key, fontsize=30)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fwXrY7zezZ_X"
      },
      "source": [
        "A quick recap on MRI basics: https://my-ms.org/mri_basics.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "70y6dcgPb54u"
      },
      "outputs": [],
      "source": [
        "# For this semantic segmentation task, we will use flair images.\n",
        "# And we will focus on binary or whole tumor (WT) segmentation. In order to do so we will assign the same pixel intensity (1) to the 3 tumor sub regions (1, 2, 4)\n",
        "\n",
        "## original code\n",
        "flair_img = pt_img_dict['flair'].get_fdata()\n",
        "seg_img = pt_img_dict['seg'].get_fdata()\n",
        "\n",
        "def normalize(input_image = flair_img,\n",
        "              input_mask = seg_img,\n",
        "              percentile = 0.001,\n",
        "              eps = 0.0000001):\n",
        "    \"\"\"\n",
        "    Will read the input image and normalize the image between 0 and 1.\n",
        "    Will read the input mask and assign pixel value 1 to all tumor subregions (1, 2, 4).\n",
        "    input_image: default is t2_img, the image to be segmented.\n",
        "    input_mask: default is seg_img, the ground truth or label.\n",
        "    \"\"\"\n",
        "    img_array = np.ndarray.flatten(input_image)\n",
        "    min_img, max_img = np.quantile(img_array, percentile), np.quantile(img_array, 1-percentile)\n",
        "    img_normalized = ((input_image-min_img)/(max_img-min_img+eps)*255).astype(np.uint8)\n",
        "    img_normalized = tf.cast(img_normalized, tf.float32) / 255\n",
        "    \n",
        "    # \n",
        "    mask_normalized = np.zeros_like(input_mask) # Create an array of zeros with the same shape as the input mask\n",
        "    #mask_normalized[input_mask == 0] = 0 # Set all values of input_mask equal to 0 to 0 in mask_normalized\n",
        "    mask_normalized[input_mask == 1] = 1 # Set all values of input_mask equal to 1 to 1 in mask_normalized\n",
        "    mask_normalized[input_mask == 2] = 2 # Set all values of input_mask equal to 2 to 2 in mask_normalized\n",
        "    mask_normalized[input_mask == 4] = 4 # Set all values of input_mask equal to 4 to 4 in mask_normalized\n",
        "    epsilon = 1e-8\n",
        "    mask_normalized = (mask_normalized - np.min(mask_normalized)) / (np.max(mask_normalized) - np.min(mask_normalized) + epsilon)\n",
        "\n",
        "\n",
        "    return img_normalized, mask_normalized\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZAeJ6RGR6Rp_"
      },
      "source": [
        "Why is image normalizaton needed: https://arthurdouillard.com/post/normalization/\n",
        "\n",
        "What are the different types of normalization: https://towardsdatascience.com/different-types-of-normalization-in-tensorflow-dac60396efb0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KqdEIQxgzpsZ"
      },
      "outputs": [],
      "source": [
        "# Let's look at the images after normalizing\n",
        "image_name = 'flair'\n",
        "input_image = pt_img_dict[image_name].get_fdata()\n",
        "img, mask = normalize(input_image) # t2_img, seg_img)\n",
        "plt.figure(figsize=(20,30))\n",
        "plt.subplot(1,4,1)\n",
        "plt.imshow(tf.transpose(input_image[:,:,78]), cmap = 'gray')\n",
        "plt.axis('off')\n",
        "plt.title('input', fontsize=20)\n",
        "plt.subplot(1,4,2)\n",
        "plt.imshow(tf.transpose(img[:,:,78]), cmap = 'gray')\n",
        "plt.axis('off')\n",
        "plt.title(image_name, fontsize=20)\n",
        "plt.subplot(1,4,3)\n",
        "plt.imshow(tf.transpose(seg_img[:,:,78]), cmap = 'gray')\n",
        "plt.axis('off')\n",
        "plt.title('mask of sub regions', fontsize=20)\n",
        "plt.subplot(1,4,4)\n",
        "plt.imshow(tf.transpose(mask[:,:,78]), cmap = 'gray')\n",
        "plt.axis('off')\n",
        "plt.title('mask of whole tumor', fontsize=20)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8mHx0NZGiciv"
      },
      "source": [
        "## Step 4: Let's take a look at the performance metrics commonly used in semantic segmentation. \n",
        "\n",
        "The 2 common ones are \n",
        "- Dice\n",
        "- Jaccard/Intersection of Union (IoU)\n",
        "\n",
        "Both Dice and Jaccard indices are bounded between 0 and 1 with 0 indicating completely inaccurate model prediction and 1 indicating completely accurate model prediction. \n",
        "\n",
        "Performance metrics in image segmentation: https://towardsdatascience.com/metrics-to-evaluate-your-semantic-segmentation-model-6bcb99639aa2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xs58zabNgbKO"
      },
      "outputs": [],
      "source": [
        "# Dice\n",
        "def dice_coef(y_true, y_pred):\n",
        "    \"\"\"\n",
        "    Dice coefficient for semantic segmentation.\n",
        "    \n",
        "    Dice = (2*|X & Y|)/ (|X|+ |Y|)\n",
        "         =  2*sum(|A*B|)/(sum(A^2)+sum(B^2))\n",
        "    \n",
        "    y_true: The ground truth tensor.\n",
        "    y_pred: The predicted tensor\n",
        "    smooth: Smoothing factor. Default is 100.    \n",
        "    \"\"\"\n",
        "    \n",
        "    y_true_f = K.flatten(y_true)\n",
        "    y_pred_f = K.flatten(y_pred)\n",
        "    intersection = K.sum(y_true_f * y_pred_f)\n",
        "    return (2. * intersection + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)\n",
        "\n",
        "# Jaccard/IoU\n",
        "def jacard_coef(y_true, y_pred):\n",
        "    \"\"\"\n",
        "    Jaccard coefficient for semantic segmentation. Also known as the IOU loss.\n",
        "    \n",
        "    Jaccard = (|X & Y|)/ (|X|+ |Y| - |X & Y|)\n",
        "            = sum(|A*B|)/(sum(|A|)+sum(|B|)-sum(|A*B|))\n",
        "    \n",
        "    y_true: The ground truth tensor.\n",
        "    y_pred: The predicted tensor\n",
        "    smooth: Smoothing factor. Default is 100.    \n",
        "    \"\"\"\n",
        "    y_true_f = K.flatten(y_true)\n",
        "    y_pred_f = K.flatten(y_pred)\n",
        "    intersection = K.sum(y_true_f * y_pred_f)\n",
        "    return (intersection + 1.0) / (K.sum(y_true_f) + K.sum(y_pred_f) - intersection + 1.0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5lJrqkPANrXo"
      },
      "source": [
        "## Step 5: Let's take a look at loss functions for semantic segmentation. \n",
        "\n",
        "The default choice of loss function for segmentation and other classification tasks is Binary Cross-Entropy (BCE). Here since the metric  Dice or Jaccard Coefficient is being used to judge model performance, the loss functions that are derived from these metrics - typically in the form 1 - f(x) where f(x) is the metric in question.\n",
        "\n",
        "Loss functions in image segmentation: https://medium.com/@junma11/loss-functions-for-medical-image-segmentation-a-taxonomy-cefa5292eec0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "46tCZ9yelSyt"
      },
      "outputs": [],
      "source": [
        "# The 2 loss functions we will look at are Dice and Jaccard loss\n",
        "\n",
        "def dice_coef_loss(y_true, y_pred):\n",
        "    return -dice_coef(y_true, y_pred)\n",
        "\n",
        "def jacard_coef_loss(y_true, y_pred):\n",
        "    return -jacard_coef(y_true, y_pred)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "64bibBDPRLOr"
      },
      "source": [
        "## Step 6: Now comes the most interesing bit of our learning!\n",
        "\n",
        "We will now build the Unet model we are going to use for semantic segmentation of WT. \n",
        "\n",
        "What is a UNet: https://towardsdatascience.com/understanding-semantic-segmentation-with-unet-6be4f42d4b47\n",
        "\n",
        "Video explaining UNet: https://www.youtube.com/watch?v=azM57JuQpQI\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uVRtFZd99jBy"
      },
      "outputs": [],
      "source": [
        "# Define the UNet model\n",
        "def get_unet(input_shape=(None, None, 1),\n",
        "                  num_classes=4):\n",
        "    inputs = Input(shape=input_shape)\n",
        "    down1 = Conv2D(64, (3, 3), activation = 'relu', padding='same')(inputs)\n",
        "    down1 = BatchNormalization()(down1)\n",
        "    down1 = Conv2D(64, (3, 3), activation = 'relu', padding='same')(down1)\n",
        "    down1 = BatchNormalization()(down1)\n",
        "    down1_pool = MaxPooling2D((2, 2), strides=(2, 2))(down1)\n",
        "  \n",
        "\n",
        "    down2 = Conv2D(128, (3, 3), activation = 'relu', padding='same')(down1_pool)\n",
        "    down2 = BatchNormalization()(down2)\n",
        "    down2 = Conv2D(128, (3, 3), activation = 'relu', padding='same')(down2)\n",
        "    down2 = BatchNormalization()(down2)\n",
        "    down2_pool = MaxPooling2D((2, 2), strides=(2, 2))(down2)\n",
        "\n",
        "\n",
        "    down3 = Conv2D(256, (3, 3), activation = 'relu', padding='same')(down2_pool)\n",
        "    down3 = BatchNormalization()(down3)\n",
        "    down3 = Conv2D(256, (3, 3), activation = 'relu', padding='same')(down3)\n",
        "    down3 = BatchNormalization()(down3)\n",
        "    down3_pool = MaxPooling2D((2, 2), strides=(2, 2))(down3)\n",
        " \n",
        "\n",
        "    down4 = Conv2D(512, (3, 3), activation = 'relu', padding='same')(down3_pool)\n",
        "    down4 = BatchNormalization()(down4)\n",
        "    down4 = Conv2D(512, (3, 3), activation = 'relu', padding='same')(down4)\n",
        "    down4 = BatchNormalization()(down4)\n",
        "    down4_pool = MaxPooling2D((2, 2), strides=(2, 2))(down4)\n",
        "\n",
        "\n",
        "    center = Conv2D(1024, (3, 3), activation = 'relu', padding='same')(down4_pool)\n",
        "    center = BatchNormalization()(center)\n",
        "    center = Conv2D(1024, (3, 3), activation = 'relu', padding='same')(center)\n",
        "    center = BatchNormalization()(center)\n",
        "\n",
        "\n",
        "    up4 = UpSampling2D((2, 2))(center)\n",
        "    up4 = concatenate([down4, up4], axis=3)\n",
        "    up4 = Conv2D(512, (3, 3), activation = 'relu', padding='same')(up4)\n",
        "    up4 = BatchNormalization()(up4)\n",
        "    up4 = Conv2D(512, (3, 3), activation = 'relu', padding='same')(up4)\n",
        "    up4 = BatchNormalization()(up4)\n",
        "    up4 = Conv2D(512, (3, 3), activation = 'relu', padding='same')(up4)\n",
        "    up4 = BatchNormalization()(up4)\n",
        "\n",
        "\n",
        "    up3 = UpSampling2D((2, 2))(up4)\n",
        "    up3 = concatenate([down3, up3], axis=3)\n",
        "    up3 = Conv2D(256, (3, 3), activation = 'relu', padding='same')(up3)\n",
        "    up3 = BatchNormalization()(up3)\n",
        "    up3 = Conv2D(256, (3, 3), activation = 'relu', padding='same')(up3)\n",
        "    up3 = BatchNormalization()(up3)\n",
        "    up3 = Conv2D(256, (3, 3), activation = 'relu', padding='same')(up3)\n",
        "    up3 = BatchNormalization()(up3)\n",
        " \n",
        "\n",
        "    up2 = UpSampling2D((2, 2))(up3)\n",
        "    up2 = concatenate([down2, up2], axis=3)\n",
        "    up2 = Conv2D(128, (3, 3), activation = 'relu', padding='same')(up2)\n",
        "    up2 = BatchNormalization()(up2)\n",
        "    up2 = Conv2D(128, (3, 3), activation = 'relu', padding='same')(up2)\n",
        "    up2 = BatchNormalization()(up2)\n",
        "    up2 = Conv2D(128, (3, 3), activation = 'relu', padding='same')(up2)\n",
        "    up2 = BatchNormalization()(up2)\n",
        "\n",
        "\n",
        "    up1 = UpSampling2D((2, 2))(up2)\n",
        "    up1 = concatenate([down1, up1], axis=3)\n",
        "    up1 = Conv2D(64, (3, 3), activation = 'relu', padding='same')(up1)\n",
        "    up1 = BatchNormalization()(up1)\n",
        "    up1 = Conv2D(64, (3, 3), activation = 'relu', padding='same')(up1)\n",
        "    up1 = BatchNormalization()(up1)\n",
        "    up1 = Conv2D(64, (3, 3), activation = 'relu', padding='same')(up1)\n",
        "    up1 = BatchNormalization()(up1)\n",
        " \n",
        "\n",
        "    classify = Conv2D(1, (1, 1), activation='sigmoid')(up1)\n",
        "\n",
        "    model = Model(inputs=inputs, outputs=classify)\n",
        "    model.compile(optimizer=Adam(learning_rate=0.001), loss=[jacard_coef_loss,dice_coef_loss], metrics=[dice_coef, jacard_coef])\n",
        "    return model\n",
        "\n",
        "model = get_unet()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Vdgj3PFKLar"
      },
      "source": [
        "Instead of constant learning rate, how to use a gradually decreasing learning rate instead: https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/LearningRateScheduler"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "je6TgYY3Tlwc"
      },
      "source": [
        "Lets visualize the model now!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AoOUrnt5-LgB"
      },
      "outputs": [],
      "source": [
        "tf.keras.utils.plot_model(model, show_shapes=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Drg3ZFNMDFvZ"
      },
      "source": [
        "## Step 7: Let's do some model training now."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mh8eSa5GUQjy"
      },
      "outputs": [],
      "source": [
        "# Let's define a function to create numpy arrays of training, validation and test images.\n",
        "\n",
        "# For the sake of simplicity, we will use only the middle slice of each volume for this experiment.\n",
        "\n",
        "X = []\n",
        "Y = []\n",
        "def create_numpy_array(patient_ids: str,\n",
        "                       DATA_PATH,\n",
        "                       slice_range: int = 1,\n",
        "                       skip_slices: int = 0):\n",
        "    X = []\n",
        "    Y = []\n",
        "    slices_to_skip = max(1, skip_slices)\n",
        "    \n",
        "    for i in range(len(patient_ids)):\n",
        "        pt_img_dict = read_patient_niftis(patient_ids[i],\n",
        "                                          niftis_to_load=['flair', 'seg'],\n",
        "                                          data_path=DATA_PATH)\n",
        "\n",
        "        images = pt_img_dict['flair']._dataobj\n",
        "        masks = pt_img_dict['seg']._dataobj\n",
        "\n",
        "        middle_slice = images.shape[-1] // 2\n",
        "\n",
        "# Will read the middle slice and choose slices before and after the middle slice \n",
        "        for k in range(middle_slice - slice_range * slices_to_skip, middle_slice + (slice_range * slices_to_skip) + 1, max(1, skip_slices)):\n",
        "            image_slice = images[:, :, k].T\n",
        "            mask_slice = masks[:, :, k].T\n",
        "\n",
        "            image, mask = normalize(image_slice, mask_slice)\n",
        "            X.append(np.expand_dims(image, axis=2))\n",
        "            Y.append(np.expand_dims(mask, axis=2))\n",
        "\n",
        "    return np.asarray(X), np.asarray(Y)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZSP5-92x21sH"
      },
      "outputs": [],
      "source": [
        "# Now using the defined function, let's create train, validation and test numpy arrays.\n",
        "#the first number is how many images to get from the middle slice & the second number is the spacing\n",
        "X_train, Y_train = create_numpy_array(train_ids, TRAIN_DATA_PATH,2,4)\n",
        "X_val, Y_val = create_numpy_array(val_ids, TRAIN_DATA_PATH, 2,4)\n",
        "X_test, Y_test = create_numpy_array(test_ids, TRAIN_DATA_PATH, 2,4)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jl1osFhfRyhw"
      },
      "source": [
        "Now we will read the .npy files and start our model training!We have a numpy arrays already created and saved"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2TQE-Mibc6ZL"
      },
      "outputs": [],
      "source": [
        "# Let's confirm the size of the training, validation and test arrays.\n",
        "print(f'There are {len(X_train)} images in training set')\n",
        "print(f'There are {len(X_val)} images in validation set')\n",
        "print(f'There are {len(X_test)} images in test set')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XzLI-YMV7UbM"
      },
      "outputs": [],
      "source": [
        "# Visualize the images\n",
        "plt.figure(figsize=(10,20))\n",
        "plt.subplot(1,2,1)\n",
        "plt.imshow(X_train[12][:,:,0], cmap = 'gray')\n",
        "plt.axis('off')\n",
        "plt.title(image_name, fontsize=20)\n",
        "plt.subplot(1,2,2)\n",
        "plt.imshow(Y_train[12][:,:,0], cmap = 'gray')\n",
        "plt.axis('off')\n",
        "plt.title('mask', fontsize=20)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vorVSRMQ5H8P"
      },
      "source": [
        "What are tensor datasets: https://tensorflow.google.cn/guide/data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z5j9Suto9fea"
      },
      "outputs": [],
      "source": [
        "# Lets create tensor datasets from numpy arrays\n",
        "dataset_train = tf.data.Dataset.from_tensor_slices((X_train, Y_train))\n",
        "dataset_train = dataset_train.map(lambda x, y: {'image': x, 'segmentation_mask': y})\n",
        "dataset_val = tf.data.Dataset.from_tensor_slices((X_val, Y_val))\n",
        "dataset_val = dataset_val.map(lambda x, y: {'image': x, 'segmentation_mask': y})\n",
        "dataset_test = tf.data.Dataset.from_tensor_slices((X_test, Y_test))\n",
        "dataset_test = dataset_test.map(lambda x, y: {'image': x, 'segmentation_mask': y})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wTsK5OYP7AnH"
      },
      "outputs": [],
      "source": [
        "# Let's create image loader to resize the image\n",
        "def load_image(datapoint):\n",
        "    input_image = tf.image.resize(datapoint['image'], (128, 128))\n",
        "    input_mask = tf.image.resize(datapoint['segmentation_mask'], (128, 128))\n",
        "\n",
        "    return input_image, input_mask"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PZQE7XEI-w1T"
      },
      "source": [
        "Understanding difference between batch and epoch in deep learning: https://machinelearningmastery.com/difference-between-a-batch-and-an-epoch/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pj8O32_D7fO8"
      },
      "outputs": [],
      "source": [
        "TRAIN_LENGTH = len(train_ids)\n",
        "BATCH_SIZE = 4\n",
        "BUFFER_SIZE = 1000\n",
        "STEPS_PER_EPOCH = TRAIN_LENGTH // BATCH_SIZE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bGc9HWZU55iH"
      },
      "source": [
        "What does AUTOTUNE do: https://www.tensorflow.org/guide/data_performance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JilpWWx97qzJ"
      },
      "outputs": [],
      "source": [
        "# Resize the image using load_image function defined earlier\n",
        "train_images = dataset_train.map(load_image, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "val_images = dataset_val.map(load_image, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "test_images = dataset_test.map(load_image, num_parallel_calls=tf.data.AUTOTUNE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7Aq8ShlIC8wb"
      },
      "outputs": [],
      "source": [
        "# Build the input pipeline, applying the Augmentation after batching the inputs.\n",
        "train_batches = (\n",
        "    train_images\n",
        "    .cache()\n",
        "    .shuffle(BUFFER_SIZE)\n",
        "    .batch(BATCH_SIZE)\n",
        "    .repeat()\n",
        "    .prefetch(buffer_size=tf.data.AUTOTUNE))\n",
        "\n",
        "val_batches = val_images.batch(BATCH_SIZE)\n",
        "test_batches = test_images.batch(BATCH_SIZE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FjLbk1bJDT45"
      },
      "outputs": [],
      "source": [
        "# Define a function to visualize an image example and its corresponding mask from the dataset.\n",
        "def display(display_list):\n",
        "    plt.figure(figsize=(15, 15))\n",
        "\n",
        "    title = ['Input Image', 'True Mask', 'Predicted Mask']\n",
        "\n",
        "    for i in range(len(display_list)):\n",
        "      plt.subplot(1, len(display_list), i+1)\n",
        "      plt.title(title[i])\n",
        "      plt.imshow(tf.keras.preprocessing.image.array_to_img(display_list[i]), cmap = 'gray')\n",
        "      plt.axis('off')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zeAtygMcDmQ8"
      },
      "outputs": [],
      "source": [
        "# Visualize an image example and its corresponding mask from the dataset batches.\n",
        "for images, masks in train_batches.take(2):\n",
        "    sample_image, sample_mask = images[0], masks[0]\n",
        "    display([sample_image, sample_mask])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UYNE02hxuBEP"
      },
      "source": [
        "Do you notice the change in image resolution of the above images as compared to the original images? The previous images look sharper. This is because we resized the images to 128 x 128. You can play with the load_image module and change the image size to see if that affects the model performance!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kbv5A6ypOP_R"
      },
      "outputs": [],
      "source": [
        "# Let's write a function to show the predictions\n",
        "def show_predictions(dataset=None, num=4):\n",
        "    if dataset:\n",
        "      for image, mask in dataset.take(num):\n",
        "        pred_mask = model.predict(image)\n",
        "        display([image[0], mask[0], pred_mask[0]])\n",
        "    else:\n",
        "      display([sample_image, sample_mask,\n",
        "              model.predict(sample_image[tf.newaxis, ...])])\n",
        "   "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yAqMnUK7EVil"
      },
      "outputs": [],
      "source": [
        "# Let's train the model now\n",
        "smooth = 1.\n",
        "EPOCHS = 20   \n",
        "VAL_SUBSPLITS = 10\n",
        "VALIDATION_STEPS = len(val_ids)//BATCH_SIZE//VAL_SUBSPLITS\n",
        "\n",
        "model_history = model.fit(train_batches, epochs=EPOCHS,\n",
        "                          steps_per_epoch=STEPS_PER_EPOCH,\n",
        "                          validation_steps=VALIDATION_STEPS,\n",
        "                          validation_data=val_batches)\n",
        "      "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sxT1JnFRdFhF"
      },
      "source": [
        "## Step 8: Now let's take a look at the learning curve. \n",
        "\n",
        "Learning curves are a widely used diagnostic tool in machine learning for algorithms that learn from a training dataset incrementally. The model can be evaluated on the training dataset and on a hold out validation dataset after each update during training and plots of the measured performance can created to show learning curves.\n",
        "\n",
        "\n",
        "Importance of learning curves: https://towardsdatascience.com/learning-curve-to-identify-overfitting-underfitting-problems-133177f38df5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fa4ih6i_FyTy"
      },
      "outputs": [],
      "source": [
        "# Let's take a look at the learning curve now\n",
        "loss = model_history.history['loss']\n",
        "val_loss = model_history.history['val_loss']\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(model_history.epoch, loss, 'r', label='Training loss')\n",
        "plt.plot(model_history.epoch, val_loss, 'b', label='Validation loss')\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.ylim([-1, 0])\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VRA_ZHx3ugOm"
      },
      "outputs": [],
      "source": [
        "# Let's also look at how the dice coef changes (Since the loss function is also derived as 1 - dice_coef, you can skip looking at this)\n",
        "jacard = model_history.history['jacard_coef']\n",
        "val_jacard = model_history.history['val_jacard_coef']\n",
        "\n",
        "# dice = model_history.history['dice_coef']\n",
        "# val_dice = model_history.history['val_dice_coef']\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(model_history.epoch, jacard, 'r', label='Training jacard')\n",
        "plt.plot(model_history.epoch, val_jacard,'b', label='Validation jacard')\n",
        "plt.title('Training and Validation Jacard')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Jacard Value')\n",
        "plt.ylim([0, 1])\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# plt.figure()\n",
        "# plt.plot(model_history.epoch, dice, 'r', label='Training dice')\n",
        "# plt.plot(model_history.epoch, val_dice,'b', label='Validation dice')\n",
        "# plt.title('Training and Validation Dice')\n",
        "# plt.xlabel('Epoch')\n",
        "# plt.ylabel('Dice Value')\n",
        "# plt.ylim([0, 1])\n",
        "# plt.legend()\n",
        "# plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7sL_83Yk8j2h"
      },
      "source": [
        "How do we choose number of epochs? Is there a way to stop training the model when it starts overfitting: https://machinelearningmastery.com/early-stopping-to-avoid-overtraining-neural-network-models/\n",
        "\n",
        "Tensorflow early stopping: https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/EarlyStopping"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ME40TLd7dWTr"
      },
      "source": [
        "## Step 9: Now we will look at the model predictions in the validation and test sets. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EmTsGnPxa2wi"
      },
      "outputs": [],
      "source": [
        "show_predictions(val_batches, 3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iiEjYAikALYn"
      },
      "outputs": [],
      "source": [
        "show_predictions(test_batches, 13)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for image, mask in test_batches.take(3):\n",
        "  predicted_mask = model.predict(image)[0]\n",
        "  true_mask = mask[0].numpy()\n",
        "  overlay = cv2.add(predicted_mask, true_mask)\n",
        "\n",
        "display([predicted_mask, true_mask])\n",
        "plt.imshow(overlay, 'BuGn'), plt.title ('Overlayed Mask')"
      ],
      "metadata": {
        "id": "w1sJK5p9RTH4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j5guE4wKdfdA"
      },
      "source": [
        "## Step 10: What next? There are so many things you could do here. So it is now time to play!\n",
        "\n",
        "As a home-work or self learning task, you can try a variety of experiments to understand how the model performance is affected. \n",
        "\n",
        "You can also do some of this as your term project.  \n",
        "\n",
        "Try to think about what could improve the model performance?\n",
        "1. Will increasing the size of training set help? We are just using 1 slice per volume, will using more slices help?\n",
        "2. Will changing the hyper-parameters like optimizer, learning rate, loss function, epochs, batch size improve the model performance? For ex. you have already learnt about BCE loss function. What will happen if you change the loss function?\n",
        "3. Will changing the type of image help? Is T2 the right image for the model? Should I try with T1/T1CE/FLAIR?\n",
        "4. Can I use 2/3 channel input image instead of single channel? Can have 3 different images in the 3 channels, say T1, T2 and FLAIR? Will that give the model more information to perform better?\n",
        "5. How is the learning curve for this trained model? Is it underfitting? Does it need more training? How much training is enough and when should I stop?\n",
        "6. Can I modify the Unet model to improve the performance? What can I change? \n",
        "\n",
        "And there is so much more! This is just a starting point. Model training and hyper-parameter tuning is the time consuming of any deep learning project. \n",
        " \n",
        "Next what?\n",
        "In this example we learnt a binary or 2 class segmentation problem. It can be extended to a multi-class problem. \n",
        "\n",
        "Another fun task to do would be to try a multi-class segmentation where you can try and predict the different sub-regions of the tumors. What will you need yo modify in the current code to change it to a multi-class problem?\n",
        "\n",
        "Are there ways to automate hyper-parameter tuning: https://neptune.ai/blog/hyperparameter-tuning-in-python-complete-guide\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YlxsauKmSz2Q"
      },
      "outputs": [],
      "source": [
        "# # Some additional functions defined for model training\n",
        "\n",
        "# # Let's look at an example of how we modify the create_numpy_array module if we want to have a multi channel image (for ex. T1, T2, FLAIR)\n",
        "\n",
        "\n",
        "# X = []\n",
        "# Y = []\n",
        "\n",
        "# def create_numpy_array(patient_ids:str,\n",
        "#                        DATA_PATH):\n",
        "#     X = []\n",
        "#     Y = []\n",
        "    \n",
        "#     for i in range(len(patient_ids)):\n",
        "#         pt_img_dict = read_patient_niftis(patient_ids[i],\n",
        "#                                           niftis_to_load = ['t1', 't2', 'flair', 'seg'], \n",
        "#                                           data_path = DATA_PATH)\n",
        "        \n",
        "#         images = []\n",
        "       \n",
        "#         for j, (key, value) in enumerate(pt_img_dict.items()):\n",
        "#             images.append(value._dataobj[:, :, value.shape[-1]//2].T)\n",
        "        \n",
        "#         images_3 = cv2.merge((images[0],images[1],images[2]))\n",
        "#         image, mask = normalize(images_3, images[3])\n",
        "#         X.append(image)\n",
        "#         Y.append(expand_dims(mask, axis=2))   \n",
        "\n",
        "#     return np.asarray(X), np.asarray(Y)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "axwuaS4ICEHx"
      },
      "outputs": [],
      "source": [
        "# # Create training, validation and test numpy arrays using bove defined function\n",
        "# X_train, Y_train = create_numpy_array(train_ids, TRAIN_DATA_PATH)\n",
        "# X_val, Y_val = create_numpy_array(val_ids, TRAIN_DATA_PATH)\n",
        "# X_test, Y_test = create_numpy_array(test_ids, TRAIN_DATA_PATH)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vfrzO7aGS705"
      },
      "outputs": [],
      "source": [
        "# # Let's look at an example of how we change the numpy array if we want to have a multiple slices per volume\n",
        "\n",
        "\n",
        "# X = []\n",
        "# Y = []\n",
        "\n",
        "# def create_numpy_array(patient_ids:str,\n",
        "#                        DATA_PATH):\n",
        "#     X = []\n",
        "#     Y = []\n",
        "    \n",
        "#     for i in range(len(patient_ids)):\n",
        "#         pt_img_dict = read_patient_niftis(patient_ids[i],\n",
        "#                                           niftis_to_load = ['t2', 'seg'], \n",
        "#                                           data_path = DATA_PATH)\n",
        "        \n",
        "#         images = []\n",
        "#         # Let's say you want 5 slices sampled every 10 slices centered around middle slice\n",
        "#         n = [-20, -10, 0, 10, 20]\n",
        "#         count = -1\n",
        "#         for k in n:\n",
        "#             for j, (key, value) in enumerate(pt_img_dict.items()):\n",
        "#                 images.append(value._dataobj[:, :, ((value.shape[-1]//2) + k)].T)\n",
        "#             count = count + 1\n",
        "               \n",
        "#             image, mask = normalize(images[count*2], images[(count*2)+1])\n",
        "#             X.append(expand_dims(image, axis=2))\n",
        "#             Y.append(expand_dims(mask, axis=2))   \n",
        "\n",
        "#     return np.asarray(X), np.asarray(Y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hkM2Auq_CR5Y"
      },
      "outputs": [],
      "source": [
        "# # Create training, validation and test numpy arrays using bove defined function\n",
        "# X_train, Y_train = create_numpy_array(train_ids, TRAIN_DATA_PATH)\n",
        "# X_val, Y_val = create_numpy_array(val_ids, TRAIN_DATA_PATH)\n",
        "# X_test, Y_test = create_numpy_array(test_ids, TRAIN_DATA_PATH)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "j5guE4wKdfdA"
      ],
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "premium",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}